{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorialFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent-space methods for uniform matrix product states\n",
    "## 1. Matrix product states in the thermodynamic limit\n",
    "### 1.1 Normalisation\n",
    "We start by considering a uniform MPS in the thermodynamic limit, which is defined by \n",
    "$$ |\\Psi(A)> = \\sum_i^d \\nu_L^\\dagger \\left[ \\prod_{m\\in Z} A_i \\right]\\nu_R |i>. $$\n",
    "\n",
    "Here, $A_i$ are complex matrices of size $D \\times D$, for every entry of the index $d$. This allows for the interpretation of the object $A$ as a three-legged tensor of dimensions $D\\times d \\times D$, where we will refer to $D$ as the bond dimension and $d$ as the physical dimension. With this object and the diagrammatic language of tensor networks, we can represent the state as\n",
    "\n",
    "![image.png](img/MPSstate.png)\n",
    "\n",
    "Thus, we initialise and represent a uniform MPS state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3 # physical dimension\n",
    "D = 5 # bond dimension\n",
    "A = np.random.rand(D, d, D) + 1j*np.random.rand(D, d, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most relevant objects in all our calculations will be the transfer matrix, defined as\n",
    "\n",
    "![image.png](img/transferMatrix.png)\n",
    "\n",
    "where we will be using the convention of ordering the legs as\n",
    "1. top left\n",
    "2. bottom left\n",
    "3. top right\n",
    "4. bottom right\n",
    "\n",
    "The transfer matrix can be shown to be a completely positive map, such that the leading eigenvalue is a positive number, which we conveniently rescale to unity for a proper normalization of the state in the thermodynamic limit. This means solving the (left and right) eigenvalue equation:\n",
    "![image.png](img/fixedPoints.png)\n",
    "\n",
    "Additionally, we may fix the normalisation of the eigenvectors by requiring that their trace is equal to one:\n",
    "![image.png](img/traceNorm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "E = np.einsum('isk,jsl->ijkl', A, np.conj(A))\n",
    "\n",
    "\"\"\"numpy eigs provides efficient way to find largest eigenvalue/vector\n",
    "    arg1 = matrix-like operator to find eigenvalue\n",
    "    k = amount of eigenvalues/vectors to return\n",
    "    which = 'LM' select largest magnitude eigenvalues\"\"\"\n",
    "\n",
    "# right fixed point\n",
    "lambdaRight, r = eigs(np.resize(E, (D**2, D**2)), k=1, which='LM')\n",
    "r.resize(D,D)\n",
    "\n",
    "# left fixed point (via transpose transfer matrix)\n",
    "lambdaLeft, l = eigs(np.resize(E, (D**2, D**2)).T, k=1, which='LM')\n",
    "l = np.resize(l, (D,D)).T # note transpose!\n",
    "\n",
    "# normalise A\n",
    "A = A / np.sqrt(lambdaRight)\n",
    "\n",
    "# normalise transfer matrix\n",
    "E = np.einsum('isk,jsl->ijkl', A, np.conj(A))\n",
    "\n",
    "# normalise fixed points\n",
    "norm = np.sqrt(np.einsum('ij,ji->', l, r))\n",
    "l = l / norm\n",
    "r = r / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(lambdaLeft - lambdaRight) < 1e-12, \"Left and right fixed point values should be the same!\"\n",
    "assert np.allclose(l, np.einsum('ijk,li,ljm->mk', A, l, np.conj(A)), 1e-12), \"l should be a left fixed point!\"\n",
    "assert np.allclose(r, np.einsum('ijk,kl,mjl->im', A, r, np.conj(A)), 1e-12), \"r should be a right fixed point!\"\n",
    "assert abs(np.einsum('ij,ji->', l, r)-1) < 1e-12, \"Left and right fixed points should be trace normalised!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now require that there is no influence of the boundary on the bulk, we require the overlap of the boundary vectors with the fixed points to equal unity. In this case, we will have properly normalised the MPS:\n",
    "![image.png](img/MPSnormalised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gauge fixing\n",
    "While a single $A$ corresponds to a unique state $|\\Psi(A)>$, the converse is not true, as different tensors may give rise to the same state. This is easily seen by noting that the gauge transform\n",
    "![image.png](img/gaugeTransform.png)\n",
    "leaves the physical state invariant. We may use this freedom in parametrization to impose canonical forms on the MPS tensor $A$.\n",
    "\n",
    "We start by considering the left-orthonormal form $A_L$ of an MPS, which has the property that\n",
    "![image.png](img/leftOrthonormal.png).\n",
    "\n",
    "We can find the matrix $L$ that brings $A$ into this form by decomposing the fixed point $l$ as $l = L^\\dagger L$, such that\n",
    "![image.png](img/leftOrthonormal2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As transfer matrix is positive definite, fixed points are also positive definite, thus allowing for a\n",
    "square root algorithm of the eigenvalues. Note that taking square roots is typically less favourable,\n",
    "as this increases the error\"\"\"\n",
    "# l = Ldag L\n",
    "S, U = np.linalg.eig(l)\n",
    "L = U@np.diag(np.sqrt(S))@np.conj(U).T\n",
    "# Al = L A L^-1\n",
    "Al = np.einsum('ij,jkl,lm->ikm', L, A, np.linalg.inv(L))\n",
    "\n",
    "assert np.allclose(np.einsum('ijk,ijl->kl', Al, np.conj(Al)), np.eye(D)), \"Al not in left-orthonormal form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, there is still room for unitary gauge transformations such that we can also bring the right fixed point in diagonal form. Similarly, we can consider the right-orthonormal form $A_R$, where we have\n",
    "![image.png](img/rightOrthonormal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = RR\n",
    "T, V = np.linalg.eig(r)\n",
    "R = V@np.diag(np.sqrt(T))@np.conj(V).T\n",
    "# Ar = R A R^-1\n",
    "Ar = np.einsum('ij,jkl,lm->ikm', np.linalg.inv(R), A, R)\n",
    "\n",
    "assert np.allclose(np.einsum('ijk,ljk->il', Ar, np.conj(Ar)), np.eye(D)), \"Ar not in right-orthonormal form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is the notion of a mixed gauge for the uniform MPS by choosing a 'center site', bringing the left tensors in left-orthonormal form and the right tensors in right-orthonormal form, and redefining the center tensor as follows:\n",
    "![image.png](img/mixedGauge.png)\n",
    "\n",
    "The mixed gauge has an intuitive interpretation. Defining $C = LR$, this implements the gauge transform that maps the left-orthonormal tensor to the right-orthonormal one, hence defining the center-site tensor $A_C$:\n",
    "![image.png](img/mixedGauge2.png)\n",
    "The above is called the gauge condition and allows us to freely move the center tensor $A_C$ around in the MPS.\n",
    "\n",
    "We can easily check that the above gauge fix holds for our tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ac = np.einsum('ij,jkl,lm->ikm', L, A, R)\n",
    "C = L@R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHS = np.einsum('ijk,kl->ijl', Al, C)\n",
    "RHS = np.einsum('ij,jkl->ikl', C, Ar)\n",
    "assert np.allclose(LHS, RHS) and np.allclose(RHS, Ac), \"Something went wrong in gauging the MPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we may perform an SVD of $C = USV^\\dagger$, and taking up $U$ and $V^\\dagger$ in the definition of $A_L$ and $A_R$, such that we are left with a diagonal $C$ on the virtual bonds.\n",
    "![image.png](img/mixedGauge3.png)\n",
    "\n",
    "In fact, this means that we can write down a Schmidt decomposition of the state across an arbitrary bond in the chain, and the diagonal elements $C_l$ are exactly the Schmidt numbers of any bipartition of the MPS. \n",
    "![image.png](img/SchmidtDecomp.png)\n",
    "Hence, we can calculate the bipartite entanglement entropy by \n",
    "$$ S = -\\sum_l C_l^2 \\log(C_l^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD\n",
    "U, S, Vdag = svd(C)\n",
    "\n",
    "# absorb unitaries in Al, Ar, Ac\n",
    "Al = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Al, U)\n",
    "Ar = np.einsum('ij,jkl,lm->ikm', Vdag, Ar, np.conj(Vdag).T)\n",
    "Ac = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Ac, np.conj(Vdag).T)\n",
    "\n",
    "# C is diagonal matrix\n",
    "C = np.diag(S)\n",
    "\n",
    "entropy = -np.sum(S**2 * np.log(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that gauging is still correct\n",
    "LHS = np.einsum('ijk,kl->ijl', Al, C)\n",
    "RHS = np.einsum('ij,jkl->ikl', C, Ar)\n",
    "assert np.allclose(LHS, RHS) and np.allclose(RHS, Ac), \"Something went wrong in gauging the MPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Truncation of a uniform MPS\n",
    "The mixed canonical form is not only useful for finding the entanglement entropy, it also allows for a way to truncate MPS states efficiently. This is done by truncating the Schmidt decomposition, such that the new MPS has a reduced bond dimension for that ond. This truncation can be shown to be optimal in the sense that the norm between the original and the truncated MPS is minimised. The truncated MPS in the mixed gauge is thus:\n",
    "![image.png](img/truncMPS.png)\n",
    "\n",
    "We mention that this is a local optimization, in the sense that it maximizes the local overlap, however not the global overlap. This would require a variational optimization of the following cost function:\n",
    "$$ || ~|\\Psi(A)> - |\\Psi(\\tilde{A})> ||^2 $$\n",
    "We postpone the detailed discussion hereof until later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate = False\n",
    "if truncate:\n",
    "    # perform an SVD\n",
    "    U, S, Vdag = svd(C)\n",
    "    \n",
    "    # truncate the SVD\n",
    "    Dprime = 2\n",
    "    U = U[:,:Dprime]\n",
    "    Vdag = Vdag[:Dprime,:]\n",
    "    S = S[:Dprime]\n",
    "    \n",
    "    # Absorb unitaries in Al, Ar, Ac\n",
    "    Al = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Al, U)\n",
    "    Ar = np.einsum('ij,jkl,lm->ikm', Vdag, Ar, np.conj(Vdag).T)\n",
    "    Ac = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Ac, np.conj(Vdag).T)\n",
    "    \n",
    "    # C is diagonal matrix\n",
    "    C = np.diag(S)\n",
    "    \n",
    "    # renormalise truncated MPS?\n",
    "    norm = np.einsum('ijk,ijk', Ac, np.conj(Ac))\n",
    "    Ac = Ac / np.sqrt(norm)\n",
    "    A = Ac\n",
    "\n",
    "# WARNING when truncated, you can no longer compare with the uniform gauge in 1.5, as A is will not be truncated\n",
    "# -> use Ac as A in this case, otherwise assert checks never fullfilled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Algorithms for finding canonical forms\n",
    "One of the great things of MPS is that they provide efficient approximations of physical states, and for gapped systems they are expected to be exact in the limit $D \\rightarrow \\infty$. The idea is that if we increase the bond dimension enough, we can get to arbitrary precision. However, increasing the bond dimension comes at a numerical cost, as the MPS algorithms scale in $D$. It is possible to ensure that the complexity of all MPS algorithms scales as $O(D^3)$, however this means we need to be a little smarter with the previously encountered algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we can refrain from explicitly building the matrices that are used in the eigenvalue solvers, but this will easily extend to all following algorithms. We can circumvent explicitly building the matrix by implementing a function that corresponds to the action of the operator, usually called a handle, and passing this to the eigenvalue solver.\n",
    "\n",
    "For example, a better way of fixing the normalisation of an MPS tensor, as well as determining left and right fixed points is by implementing the handles and using the optimal contraction sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimal contraction sequences\n",
    "if False:\n",
    "    x = np.ones(D**2)\n",
    "    A = np.ones((D,d,D))\n",
    "    path = np.einsum_path('ijk,ljm,km->il', A, np.conj(A), x.reshape((D, D)), optimize='optimal')\n",
    "    print('optimal path for right fixed point contraction: ', path[0])\n",
    "    \n",
    "    path2 = np.einsum_path('ijk,ljm,li->mk', A, np.conj(A), x.reshape((D, D)), optimize='optimal')\n",
    "    print('optimal path for left fixed point contraction: ', path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftFixedPoint(A):\n",
    "    \"\"\"\n",
    "    Function to determine the left fixed point of a given MPS tensor using O(D^3) algorithm\n",
    "    input: A --- (D, d, D) MPStensor\n",
    "    output: l --- (D, D) leftFixedPointTensor (bottom-top)\n",
    "            lam --- scalar eigenvalue of leftFixedPointTensor\n",
    "    \"\"\"\n",
    "\n",
    "    D = A.shape[0]\n",
    "\n",
    "    # set optimal contraction sequence\n",
    "    path = ['einsum_path', (0, 2), (0, 1)]\n",
    "\n",
    "    # calculate transfer matrix handle and cast to LinearOperator\n",
    "    transferLeftHandle = lambda v: np.reshape(\n",
    "        np.einsum('ijk,ljm,li->mk', A, np.conj(A), v.reshape((D, D)), optimize=path), D ** 2)\n",
    "    transferLeft = LinearOperator((D ** 2, D ** 2), matvec=transferLeftHandle)\n",
    "\n",
    "    # calculate fixed point\n",
    "    lam, l = eigs(transferLeft, k=1, which='LM')\n",
    "\n",
    "    return lam, l.reshape(D, D)\n",
    "\n",
    "\n",
    "def rightFixedPoint(A):\n",
    "    \"\"\"\n",
    "    Function to determine the right fixed point of a given MPS tensor using O(D^3) algorithm\n",
    "    input: A --- (D, d, D) MPStensor\n",
    "    output: r --- (D, D) rightFixedPointTensor (top-bottom)\n",
    "            lam --- scalar eigenvalue of rightFixedPointTensor\n",
    "    \"\"\"\n",
    "\n",
    "    D = A.shape[0]\n",
    "\n",
    "    # set optimal contraction sequence\n",
    "    path = ['einsum_path', (0, 2), (0, 1)]\n",
    "\n",
    "    # calculate transfer matrix handle and cast to LinearOperator\n",
    "    transferRightHandle = lambda v: np.reshape(\n",
    "        np.einsum('ijk,ljm,km->il', A, np.conj(A), v.reshape((D, D)), optimize=path), D ** 2)\n",
    "    transferRight = LinearOperator((D ** 2, D ** 2), matvec=transferRightHandle)\n",
    "\n",
    "    # calculate fixed point\n",
    "    lam, r = eigs(transferRight, k=1, which='LM')\n",
    "\n",
    "    return lam, r.reshape(D, D)\n",
    "\n",
    "\n",
    "def fixedPoints(A):\n",
    "    return leftFixedPoint(A), rightFixedPoint(A)\n",
    "\n",
    "\n",
    "def normaliseFixedPoints(l, r):\n",
    "    # function that normalises given left and right fixed points\n",
    "    # such that they trace to unity (interpretation as density matrix)\n",
    "    # returns (l, r)\n",
    "\n",
    "    trace = np.einsum('ij,ji->', l, r)\n",
    "    # trace = np.trace(rhoL*rhoR) # might work as well/be faster than einsum?\n",
    "    norm = np.sqrt(trace)\n",
    "    return l/norm, r/norm\n",
    "\n",
    "\n",
    "def normaliseMPS(A):\n",
    "    \"\"\"\n",
    "    Function to normalise a given MPS tensor using O(D^3) algorithm\n",
    "    input: A --- (D, d, D) MPStensor\n",
    "    output: A --- (D, d, D) normalised MPStensor\n",
    "            l --- (D, D) leftFixedPointTensor (bottom-top)\n",
    "            r --- (D, D) rightFixedPointTensor (top-bottom)\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate left and right fixed points of transfer matrix\n",
    "    lam, l = leftFixedPoint(A)\n",
    "    r = rightFixedPoint(A)[1]\n",
    "    \n",
    "    # normalise MPS tensor\n",
    "    A /= np.sqrt(lam)\n",
    "    \n",
    "    # normalise fixed points\n",
    "    l, r = normaliseFixedPoints(l, r)\n",
    "\n",
    "    return A, l, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, l, r = normaliseMPS(A)\n",
    "assert np.allclose(l, np.einsum('ijk,li,ljm->mk', A, l, np.conj(A)), 1e-12), \"l should be a left fixed point!\"\n",
    "assert np.allclose(r, np.einsum('ijk,kl,mjl->im', A, r, np.conj(A)), 1e-12), \"r should be a right fixed point!\"\n",
    "assert abs(np.einsum('ij,ji->', l, r)-1) < 1e-12, \"Left and right fixed points should be trace normalised!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, taking the square root of such a fixed point to find the left and right gauge is also a way to introduce larger errors than neccesary. This can be circumvented by using an iterative scheme that makes use of a unique decomposition, such as a QR or polar decomposition. Note however that for a QR decomposition to be unique we need the diagonal elements of R to have a positive sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrPositive(A):\n",
    "    \"\"\"\n",
    "    Function that implements a QR decomposition of a matrix A,\n",
    "    such that A = QR\n",
    "    input: A --- matrix (M, N)\n",
    "    output: Q --- matrix (M, N), isometry\n",
    "            R --- matrix (N, N), upper triangular, positive diagonal elements\n",
    "    \"\"\"\n",
    "    \n",
    "    M, N = A.shape\n",
    "    \n",
    "    # QR decomposition, scipy conventions: Q.shape = (M, M), R.shape = (M, N)\n",
    "    Q, R = qr(A)\n",
    "    \n",
    "    # Throw out zeros under diagonal: Q.shape = (M, N), R.shape = (N, N)\n",
    "    Q = Q[:, :N]\n",
    "    R = R[:N, :]\n",
    "\n",
    "    # extract signs and multiply with signs on diagonal\n",
    "    diagSigns = np.diag(np.sign(np.diag(R)))\n",
    "    Q = np.dot(Q, diagSigns)\n",
    "    R = np.dot(diagSigns, R)\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "\n",
    "def lqPositive(A):\n",
    "    \"\"\"\n",
    "    Function that implements a LQ decomposition of a matrix A,\n",
    "    such that A = LQ\n",
    "    input: A --- matrix (M, N)\n",
    "    output: L --- matrix (M, M), upper triangular, positive diagonal elements\n",
    "            Q --- matrix (M, N), isometry   \n",
    "    \"\"\"\n",
    "    \n",
    "    M, N = A.shape\n",
    "    \n",
    "    # LQ decomposition: scipy conventions: Q.shape = (N, N), L.shape = (M, N)\n",
    "    L, Q = rq(A)\n",
    "\n",
    "    # Throw out zeros under diagonal: Q.shape = (M, N), L.shape = (M, M)\n",
    "    Q = Q[-M:, :]\n",
    "    L = L[:, -M:]\n",
    "\n",
    "    # Extract signs and multiply with signs on diagonal\n",
    "    diagSigns = np.diag(np.sign(np.diag(L)))\n",
    "    Q = np.dot(diagSigns, Q)\n",
    "    L = np.dot(L, diagSigns)\n",
    "\n",
    "    return L, Q\n",
    "\n",
    "\n",
    "def leftOrthonormal(A, L0=None, tol=1e-14, maxIter=1e5):\n",
    "    \"\"\"\n",
    "    Function that brings MPS into left-orthonormal gauge,\n",
    "    such that -L-A- = -Al-L-\n",
    "    input: A --- MPSTensor (D, d, D)\n",
    "            L0 --- initial guess for L\n",
    "            tol --- convergence tolerance\n",
    "            maxIter --- max amount of steps\n",
    "    output: L --- Matrix (D, D) gauges A to Al\n",
    "            Al --- MPSTensor (D, d, D) left-orthonormal, -conj(Al)=Al- = --\n",
    "    \"\"\"\n",
    "\n",
    "    D = A.shape[0]\n",
    "    d = A.shape[1]\n",
    "    i = 1\n",
    "\n",
    "    # Random guess for L0 if none specified\n",
    "    if L0 is None:\n",
    "        L0 = np.random.rand(D, D)\n",
    "\n",
    "    # Normalise L0\n",
    "    L0 = L0 / np.linalg.norm(L0)\n",
    "\n",
    "    # Initialise loop\n",
    "    Al, L = qrPositive(np.resize(np.einsum('ik,ksj->isj', L0, A), (D * d, D)))\n",
    "    L = L / np.linalg.norm(L)\n",
    "    convergence = np.linalg.norm(L - L0)\n",
    "\n",
    "    # Decompose L*A until L converges\n",
    "    while convergence > tol:\n",
    "        # calculate LA and decompose\n",
    "        Al, Lnew = qrPositive(np.resize(np.einsum('ik,ksj->isj', L, A), (D * d, D)))\n",
    "\n",
    "        # normalise new L\n",
    "        Lnew = Lnew / np.linalg.norm(Lnew)  # only necessary when working with unnormalised MPS?\n",
    "\n",
    "        # calculate convergence criterium\n",
    "        convergence = np.linalg.norm(Lnew - L)\n",
    "        L = Lnew\n",
    "\n",
    "        # check if iterations exceeds maxIter\n",
    "        if i > maxIter:\n",
    "            print(\"Warning, decomposition has not converged \", convergence)\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return L, np.resize(Al, (D, d, D))\n",
    "\n",
    "\n",
    "def rightOrthonormal(A, R0=None, tol=1e-14, maxIter=1e5):\n",
    "    \"\"\"\n",
    "    Function that brings MPS into right-orthonormal gauge,\n",
    "    such that -A-R- = -R-Ar-\n",
    "    input: A --- MPSTensor (D, d, D)\n",
    "            R0 --- initial guess for R\n",
    "            tol --- convergence tolerance\n",
    "            maxIter --- max amount of steps\n",
    "    output: R --- Matrix (D, D) gauges A to Al\n",
    "            Ar --- MPSTensor (D, d, D) right-orthonormal, -conj(Ar)=Ar- = --\n",
    "    \"\"\"\n",
    "\n",
    "    D = A.shape[0]\n",
    "    d = A.shape[1]\n",
    "    i = 1\n",
    "\n",
    "    # Random guess for  R0 if none specified\n",
    "    if R0 is None:\n",
    "        R0 = np.random.rand(D, D)\n",
    "\n",
    "    # Normalise R0\n",
    "    R0 = R0 / np.linalg.norm(R0)\n",
    "\n",
    "    # Initialise loop\n",
    "    R, Ar = lqPositive(np.resize(np.einsum('ijk,kl->ijl', A, R0), (D, D * d)))\n",
    "    R = R / np.linalg.norm(R)\n",
    "    convergence = np.linalg.norm(R - R0)\n",
    "\n",
    "    # Decompose A*R until R converges\n",
    "    while convergence > tol:\n",
    "        # calculate AR and decompose\n",
    "        Rnew, Ar = lqPositive(np.resize(np.einsum('ijk,kl->ijl', A, R), (D, D * d)))\n",
    "\n",
    "        # normalise new R\n",
    "        Rnew = Rnew / np.linalg.norm(Rnew)  # only necessary when working with unnormalised MPS\n",
    "\n",
    "        # calculate convergence criterium\n",
    "        convergence = np.linalg.norm(Rnew - R)\n",
    "        R = Rnew\n",
    "\n",
    "        # check if iterations exceeds maxIter\n",
    "        if i > maxIter:\n",
    "            print(\"Warning, decomposition has not converged \", convergence)\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return R, np.resize(Ar, (D, d, D))\n",
    "\n",
    "\n",
    "def mixedCanonical(A, L0=None, R0=None, tol=1e-14, maxIter=1e5):\n",
    "    \"\"\"\n",
    "    Function that brings MPS into mixed gauge,\n",
    "    such that -Al-C- = -C-Ar- = Ac\n",
    "    input:  A --- MPSTensor (D, d, D)\n",
    "            L0 --- initial guess for L\n",
    "            R0 --- initial guess for R\n",
    "            tol --- convergence tolerance\n",
    "            maxIter --- max amount of steps\n",
    "    output: Al --- MPSTensor (D, d, D) left-orthonormal, -conj(Al)=Al- = --\n",
    "            Ar --- MPSTensor (D, d, D) right-orthonormal, -conj(Ar)=Ar- = --\n",
    "            Ac --- MPSTensor (D, d, D) center tensor\n",
    "            C --- Matrix (D, D) center matrix, -Al-C- = -C-Ar- = Ac\n",
    "    \"\"\"\n",
    "\n",
    "    D = A.shape[0]\n",
    "\n",
    "    # Random guess for  L0 if none specified\n",
    "    if L0 is None:\n",
    "        L0 = np.random.rand(D, D)\n",
    "\n",
    "    # Random guess for  R0 if none specified\n",
    "    if R0 is None:\n",
    "        R0 = np.random.rand(D, D)\n",
    "    \n",
    "    # Compute left and right orthonormal forms\n",
    "    L, Al = leftOrthonormal(A, L0, tol, maxIter)\n",
    "    R, Ar = rightOrthonormal(A, L0, tol, maxIter)\n",
    "    \n",
    "    # center matrix C is matrix multiplication of L and R\n",
    "    C = L @ R\n",
    "    \n",
    "    # singular value decomposition to diagonalise C\n",
    "    U, S, Vdag = svd(C)\n",
    "    C = np.diag(S)\n",
    "\n",
    "    # absorb corresponding unitaries in Al and Ar\n",
    "    Al = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Al, U)\n",
    "    Ar = np.einsum('ij,jkl,lm->ikm', Vdag, Ar, np.conj(Vdag).T)\n",
    "\n",
    "    # compute center MPS tensor\n",
    "    Ac = np.einsum('ijk,kl->ijl', Al, C)\n",
    "    \n",
    "    # normalise MPS in mixed gauge\n",
    "    nrm = np.einsum('ijk,ijk', Ac, np.conj(Ac))\n",
    "    Ac /= np.sqrt(nrm);\n",
    "    \n",
    "    return Al, Ar, Ac, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Al, Ar, Ac, C = mixedCanonical(A, L0=None, R0=None, tol=1e-14, maxIter=1e5)\n",
    "assert np.allclose(np.einsum('ijk,ijl->kl', Al, np.conj(Al)), np.eye(D)), \"Al1 not in left-orthonormal form\"\n",
    "assert np.allclose(np.einsum('ijk,ljk->il', Ar, np.conj(Ar)), np.eye(D)), \"Ar not in right-orthonormal form\"\n",
    "LHS = np.einsum('ijk,kl->ijl', Al, C)\n",
    "RHS = np.einsum('ij,jkl->ikl', C, Ar)\n",
    "assert np.allclose(LHS, RHS) and np.allclose(RHS/np.sqrt(np.einsum('ijk,ijk', RHS, np.conj(RHS))), Ac), \"Something went wrong in gauging the MPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "For the remainder of this tutorial, keep in mind that you should always aim to reduce the complexity of your algorithm to $O(D^3)$ in order to keep the computational cost tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Computing expectation values \n",
    "Having described the states in different gauges, we want to use these to determine the expectation values of extensive operators:\n",
    "$$ O = \\frac{1}{Z} \\sum_{n\\in Z} O_n $$\n",
    "\n",
    "If we work with properly normalized MPS, the expectation value per site of a one-body operator (such as an order parameter e.g. $X_i$) is then found by considering the following contraction:\n",
    "![image.png](img/expValue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.random.rand(d,d) + 1.0j*np.random.rand(d,d)\n",
    "# convention from top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the uniform gauge, we can use the fixed points of the transfer matrix to collapse everything on the left and right, such that we are left with the contraction:\n",
    "![image.png](img/expValue2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.8710873507348424+1.5551861153624327j)\n"
     ]
    }
   ],
   "source": [
    "expVal = oneSiteUniform(O, A, l, r)\n",
    "print(expVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the mixed gauge, we can locate the center site where the operator is acting, and then contract everything to the left and right to the identity, such that we find\n",
    "![image.png](img/expValue3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.871087350734847+1.5551861153624247j)\n"
     ]
    }
   ],
   "source": [
    "expValMix = oneSiteMixed(O, Ac)\n",
    "print(expValMix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if exp values are the same in different gauges\n",
    "diff = abs(expVal - expValMix)\n",
    "assert diff < 1e-14, \"different gauges give different values?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure generalises easily to operators that act on multiple sites. In particular, the Hamiltonian $h$ can be evaluated as\n",
    "![image.png](img/hamExpVal.png)\n",
    "#this is not correct, the contribution where Ac stands on the right is missing, i think it is, this is the Hamiltonian density, not the complete hamiltonian so there is only a single contribution. That or i dont understand what you mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.9878470797955999-5.421010862427522e-17j)\n",
      "(-0.9878470797955928-7.922368163212073e-17j)\n",
      "(-0.9878470797955954+1.0845550519353461e-17j)\n"
     ]
    }
   ],
   "source": [
    "Jx, Jy, Jz, h = 1, 1, 1, 1\n",
    "H = Heisenberg(Jx, Jy, Jz, h)\n",
    "\n",
    "expVal = twoSiteUniform(H, A, l, r)\n",
    "expValGauge = twoSiteMixed(H, Ac, Ar)\n",
    "expValGauge2 = twoSiteMixed(H, Al, Ac)\n",
    "\n",
    "print(expVal)\n",
    "print(expValGauge)\n",
    "print(expValGauge2)\n",
    "diff1 = abs(expVal - expValGauge)\n",
    "diff2 = abs(expVal - expValGauge2)\n",
    "\n",
    "assert diff1 < 1e-14 and diff2 < 1e-14, \"different gauges give different values?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Summary"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
