{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorialFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent-space methods for uniform matrix product states\n",
    "## 1. Matrix product states in the thermodynamic limit\n",
    "### 1.1 Normalisation\n",
    "We start by considering a uniform MPS in the thermodynamic limit, which is defined by \n",
    "$$ |\\Psi(A)> = \\sum_i^d \\nu_L^\\dagger \\left[ \\prod_{m\\in Z} A_i \\right]\\nu_R |i>. $$\n",
    "\n",
    "Here, $A_i$ are complex matrices of size $D \\times D$, for every entry of the index $d$. This allows for the interpretation of the object $A$ as a three-legged tensor of dimensions $D\\times d \\times D$, where we will refer to $D$ as the bond dimension and $d$ as the physical dimension. With this object and the diagrammatic language of tensor networks, we can represent the state as\n",
    "\n",
    "![image.png](img/MPSstate.png)\n",
    "\n",
    "Thus, we initialise and represent a uniform MPS state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3 # physical dimension\n",
    "D = 5 # bond dimension\n",
    "A = np.random.rand(D, d, D) + 1j*np.random.rand(D, d, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most relevant objects in all our calculations will be the transfer matrix, defined as\n",
    "\n",
    "![image.png](img/transferMatrix.png)\n",
    "\n",
    "where we will be using the convention of ordering the legs as\n",
    "1. top left\n",
    "2. bottom left\n",
    "3. top right\n",
    "4. bottom right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "E = np.einsum('isk,jsl->ijkl', A, np.conj(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transfer matrix can be shown to be a completely positive map, such that the leading eigenvalue is a positive number, which we conveniently rescale to unity for a proper normalization of the state in the thermodynamic limit. This means solving the (left and right) eigenvalue equation:\n",
    "![image.png](img/fixedPoints.png)\n",
    "\n",
    "Additionally, we may fix the normalisation of the eigenvectors by requiring that their trace is equal to one:\n",
    "![image.png](img/traceNorm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"numpy eigs provides efficient way to find largest eigenvalue/vector\n",
    "    arg1 = matrix-like operator to find eigenvalue\n",
    "    k = amount of eigenvalues/vectors to return\n",
    "    which = 'LM' select largest magnitude eigenvalues\"\"\"\n",
    "\n",
    "# right fixed point\n",
    "lambdaRight, r = eigs(np.resize(E, (D**2, D**2)), k=1, which='LM')\n",
    "r.resize(D,D)\n",
    "\n",
    "# left fixed point (via transpose transfer matrix)\n",
    "lambdaLeft, l = eigs(np.resize(E, (D**2, D**2)).T, k=1, which='LM')\n",
    "l = np.resize(l, (D,D)).T # note transpose!\n",
    "\n",
    "# normalise A\n",
    "A = A / np.sqrt(lambdaRight)\n",
    "\n",
    "# normalise transfer matrix\n",
    "E = np.einsum('isk,jsl->ijkl', A, np.conj(A))\n",
    "\n",
    "# normalise fixed points\n",
    "norm = np.sqrt(np.einsum('ij,ji->', l, r))\n",
    "l = l / norm\n",
    "r = r / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(lambdaLeft - lambdaRight) < 1e-12, \"Left and right fixed point values should be the same!\"\n",
    "assert np.allclose(l, np.einsum('ijk,li,ljm->mk', A, l, np.conj(A)), 1e-12), \"l should be a left fixed point!\"\n",
    "assert np.allclose(r, np.einsum('ijk,kl,mjl->im', A, r, np.conj(A)), 1e-12), \"r should be a right fixed point!\"\n",
    "assert abs(np.einsum('ij,ji->', l, r)-1) < 1e-12, \"Left and right fixed points should be trace normalised!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now require that there is no influence of the boundary on the bulk, we require the overlap of the boundary vectors with the fixed points to equal unity. In this case, we will have properly normalised the MPS:\n",
    "![image.png](img/MPSnormalised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gauge fixing\n",
    "While a single $A$ corresponds to a unique state $|\\Psi(A)>$, the converse is not true, as different tensors may give rise to the same state. This is easily seen by noting that the gauge transform\n",
    "![image.png](img/gaugeTransform.png)\n",
    "leaves the physical state invariant. We may use this freedom in parametrization to impose canonical forms on the MPS tensor $A$.\n",
    "\n",
    "We start by considering the left-orthonormal form $A_L$ of an MPS, which has the property that\n",
    "![image.png](img/leftOrthonormal.png).\n",
    "\n",
    "We can find the matrix $L$ that brings $A$ into this form by decomposing the fixed point $l$ as $l = L^\\dagger L$, such that\n",
    "![image.png](img/leftOrthonormal2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As transfer matrix is positive definite, fixed points are also positive definite, thus allowing for a\n",
    "square root algorithm of the eigenvalues. Note that taking square roots is typically less favourable,\n",
    "as this increases the error\"\"\"\n",
    "# l = LL\n",
    "S, U = np.linalg.eig(l)\n",
    "L = U@np.diag(np.sqrt(S))@np.conj(U).T\n",
    "# Al = L A L^-1\n",
    "Al = np.einsum('ij,jkl,lm->ikm', L, A, np.linalg.inv(L))\n",
    "\n",
    "assert np.allclose(np.einsum('ijk,ijl->kl', Al, np.conj(Al)), np.eye(D)), \"Al1 not in left-orthonormal form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, there is still room for unitary gauge transformations such that we can also bring the right fixed point in diagonal form. Similarly, we can consider the right-orthonormal form $A_R$, where we have\n",
    "![image.png](img/rightOrthonormal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = RR\n",
    "T, V = np.linalg.eig(r)\n",
    "R = V@np.diag(np.sqrt(T))@np.conj(V).T\n",
    "# Ar = R A R^-1\n",
    "Ar = np.einsum('ij,jkl,lm->ikm', np.linalg.inv(R), A, R)\n",
    "\n",
    "assert np.allclose(np.einsum('ijk,ljk->il', Ar, np.conj(Ar)), np.eye(D)), \"Ar not in right-orthonormal form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is the notion of a mixed gauge for the uniform MPS by choosing a 'center site', bringing the left tensors in left-orthonormal form and the right tensors in right-orthonormal form, and redefining the center tensor as follows:\n",
    "![image.png](img/mixedGauge.png)\n",
    "\n",
    "The mixed gauge has an intuitive interpretation. Defining $C = LR$, this implements the gauge transform that maps the left-orthonormal tensor to the right-orthonormal one, hence defining the center-site tensor $A_C$:\n",
    "![image.png](img/mixedGauge2.png)\n",
    "The above is called the gauge condition and allows us to freely move the center tensor $A_C$ around in the MPS.\n",
    "\n",
    "We can easily check that the above gauge fix holds for our tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ac = np.einsum('ij,jkl,lm->ikm', L, A, R)\n",
    "C = L@R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHS = np.einsum('ijk,kl->ijl', Al, C)\n",
    "RHS = np.einsum('ij,jkl->ikl', C, Ar)\n",
    "assert np.allclose(LHS, RHS) and np.allclose(RHS, Ac), \"Something went wrong in gauging the MPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we may perform an SVD of $C = USV^\\dagger$, and taking up $U$ and $V^\\dagger$ in the definition of $A_L$ and $A_R$, such that we are left with a diagonal $C$ on the virtual bonds.\n",
    "![image.png](img/mixedGauge3.png)\n",
    "\n",
    "In fact, this means that we can write down a Schmidt decomposition of the state across an arbitrary bond in the chain, and the diagonal elements $C_l$ are exactly the Schmidt numbers of any bipartition of the MPS. \n",
    "![image.png](img/SchmidtDecomp.png)\n",
    "Hence, we can calculate the bipartite entanglement entropy by \n",
    "$$ S = -\\sum_l C_l^2 \\log(C_l^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD\n",
    "U, S, Vdag = svd(C)\n",
    "\n",
    "# absorb unitaries in Al, Ar, Ac\n",
    "Al = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Al, U)\n",
    "Ar = np.einsum('ij,jkl,lm->ikm', Vdag, Ar, np.conj(Vdag).T)\n",
    "Ac = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Ac, np.conj(Vdag).T)\n",
    "\n",
    "# C is diagonal matrix\n",
    "C = np.diag(S)\n",
    "\n",
    "entropy = -np.sum(S**2 * np.log(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that gauging is still correct\n",
    "LHS = np.einsum('ijk,kl->ijl', Al, C)\n",
    "RHS = np.einsum('ij,jkl->ikl', C, Ar)\n",
    "assert np.allclose(LHS, RHS) and np.allclose(RHS, Ac), \"Something went wrong in gauging the MPS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Truncation of a uniform MPS\n",
    "The mixed canonical form is not only useful for finding the entanglement entropy, it also allows for a way to truncate MPS states efficiently. This is done by truncating the Schmidt decomposition, such that the new MPS has a reduced bond dimension for that ond. This truncation can be shown to be optimal in the sense that the norm between the original and the truncated MPS is minimised. The truncated MPS in the mixed gauge is thus:\n",
    "![image.png](img/truncMPS.png)\n",
    "\n",
    "We mention that this is a local optimization, in the sense that it maximizes the local overlap, however not the global overlap. This would require a variational optimization of the following cost function:\n",
    "$$ || ~|\\Psi(A)> - |\\Psi(\\tilde{A})> ||^2 $$\n",
    "We postpone the detailed discussion hereof until later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate = False\n",
    "if truncate:\n",
    "    # perform an SVD\n",
    "    U, S, Vdag = svd(C)\n",
    "    \n",
    "    # truncate the SVD\n",
    "    Dprime = 2\n",
    "    U = U[:,:Dprime]\n",
    "    Vdag = Vdag[:Dprime,:]\n",
    "    S = S[:Dprime]\n",
    "    \n",
    "    # Absorb unitaries in Al, Ar, Ac\n",
    "    Al = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Al, U)\n",
    "    Ar = np.einsum('ij,jkl,lm->ikm', Vdag, Ar, np.conj(Vdag).T)\n",
    "    Ac = np.einsum('ij,jkl,lm->ikm', np.conj(U).T, Ac, np.conj(Vdag).T)\n",
    "    \n",
    "    # C is diagonal matrix\n",
    "    C = np.diag(S)\n",
    "    \n",
    "    # renormalise truncated MPS?\n",
    "    norm = np.einsum('ijk,ijk', Ac, np.conj(Ac))\n",
    "    Ac = Ac / np.sqrt(norm)\n",
    "    A = Ac\n",
    "\n",
    "# WARNING when truncated, you can no longer compare with the uniform gauge in 1.5, as A is will not be truncated\n",
    "# -> use Ac as A in this case, otherwise assert checks never fullfilled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Algorithms for finding canonical forms\n",
    "One of the great things of MPS is that they provide efficient approximations of physical states, and for gapped systems they are expected to be exact in the limit $D \\rightarrow \\infty$. The idea is that if we increase the bond dimension enough, we can get to arbitrary precision. However, increasing the bond dimension comes at a numerical cost, as the MPS algorithms scale in $D$. It is possible to ensure that the complexity of all MPS algorithms scales as $O(D^3)$, however this means we need to be a little smarter with the previously encountered algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO show and explain the 'smarter algorithms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "For the remainder of this tutorial, keep in mind that you should always aim to reduce the complexity of your algorithm to $O(D^3)$ in order to keep the computational cost tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Computing expectation values \n",
    "Having described the states in different gauges, we want to use these to determine the expectation values of extensive operators:\n",
    "$$ O = \\frac{1}{Z} \\sum_{n\\in Z} O_n $$\n",
    "\n",
    "If we work with properly normalized MPS, the expectation value per site of a one-body operator (such as an order parameter e.g. $X_i$) is then found by considering the following contraction:\n",
    "![image.png](img/expValue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.random.rand(d,d) + 1.0j*np.random.rand(d,d)\n",
    "# convention from top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the uniform gauge, we can use the fixed points of the transfer matrix to collapse everything on the left and right, such that we are left with the contraction:\n",
    "![image.png](img/expValue2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.3632473221169887+1.7060768395283918j)\n"
     ]
    }
   ],
   "source": [
    "expVal = oneSiteUniform(O, A, l, r)\n",
    "print(expVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the mixed gauge, we can locate the center site where the operator is acting, and then contract everything to the left and right to the identity, such that we find\n",
    "![image.png](img/expValue3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.363247322116985+1.7060768395283905j)\n"
     ]
    }
   ],
   "source": [
    "expValMix = oneSiteMixed(O, Ac)\n",
    "print(expValMix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if exp values are the same in different gauges\n",
    "diff = abs(expVal - expValMix)\n",
    "assert diff < 1e-14, \"different gauges give different values?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure generalises easily to operators that act on multiple sites. In particular, the Hamiltonian $h$ can be evaluated as\n",
    "![image.png](img/hamExpVal.png)\n",
    "#this is not correct, the contribution where Ac stands on the right is missing, i think it is, this is the Hamiltonian density, not the complete hamiltonian so there is only a single contribution. That or i dont understand what you mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.8507247631530352+1.9949319973733282e-17j)\n",
      "(-0.850724763153035-1.5983022685510506e-16j)\n",
      "(-0.8507247631530352+5.817957550316918e-17j)\n"
     ]
    }
   ],
   "source": [
    "Jx, Jy, Jz, h = 1, 1, 1, 1\n",
    "H = Heisenberg(Jx, Jy, Jz, h)\n",
    "\n",
    "expVal = twoSiteUniform(H, A, l, r)\n",
    "expValGauge = twoSiteMixed(H, Ac, Ar)\n",
    "expValGauge2 = twoSiteMixed(H, Al, Ac)\n",
    "\n",
    "print(expVal)\n",
    "print(expValGauge)\n",
    "print(expValGauge2)\n",
    "diff1 = abs(expVal - expValGauge)\n",
    "diff2 = abs(expVal - expValGauge2)\n",
    "\n",
    "assert diff1 < 1e-14 and diff2 < 1e-14, \"different gauges give different values?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Summary"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
